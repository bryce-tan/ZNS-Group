# 阅读记录

## ZNS: Avoiding the Block Interface Tax for Flash-based SSDs

### 背景

传统接口的一些特性：

1. 传统的SSD和主机软件的功能划分:![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611130607424-554407542.png)其中SSD的FTL层(Flash Translation Layer，闪存翻译层)需要有映射和垃圾回收机制且SSD要有一定的预留空间
2. 块接口和当前存储设备特性(闪存颗粒)之间存在明显的不匹配，这种不匹配主要体现在我们可以将块写入闪存(接口决定的)但在擦除时必须以更大的粒度擦除闪存(闪存颗粒的性质决定的)。传统SSD使用块接口，导致其要想减少垃圾回收相应的主机软件会较为复杂，用于页面映射的DRAM较大，预留空间也较大(为了减少垃圾回收)，且仍然可能导致吞吐量限制 [17]、写入放大 [3]、性能不可预测性 [33， 64] 和高尾延迟 [16]

#### SSD

[SSD特性的来源](https://www.tonguebusy.com/a/peixun/xinxi/03-we-q-w-06.html)

* 逻辑块的大小一般是一页，一个擦除块可能包含多个页![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611134729590-1304353693.png)![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611142220405-1672501370.png)
* ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614175145312-805185323.png)

为了给基于闪存的设备一个块接口，我们需要FTL层来让设备支持随机"覆写"，映射，垃圾回收，均匀使用闪存

一些缺点：
> 闪存颗粒不支持"覆写"，它只能将数据擦除后重新写，所以逻辑块的写入实际上是写入到一个没有数据的地方，且需要旧的不会用到的数据所在的擦除块被擦除来腾出空间。当擦除块中有的数据用不到了，有的数据还需要使用，那么我们得将需要使用的数据转移到其他擦除块中，把其他擦除块中要擦除的数据转移到那个将要擦除的块统一擦除，为此我们需要预留一部分空间(7%或28%)来方便转移数据。![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611142321342-1853616515.png)
> 首先谁也不能保证某些数据将来一定用不到了，垃圾回收机制只能按照某种标准来决定回收哪些数据，这会带来操作性能的不确定性
> 其次剩余容量越小，垃圾回收就会越频繁，而垃圾回收需要转移数据和初始化擦除块，这会带来一定的写放大
> 再次SSD的使用寿命等同于Block的擦写次数，为了提升总体寿命，SSD的主控芯片会尽可能的让每个Block的擦写次数均匀。这就需要擦写特定的块，也会导致一定的写放大。
> 再再次由于逻辑块的粒度较小，为了支持映射需要较大的DRAM来保存页表
> 最后是预留空间的代价，它不能用来存数据

现有的两种改进措施

1. Stream SSD
2. Open-Channel SSD

Stream SSD将传入数据分到不同的擦除块，从而提升整体的性能和寿命。做法是主机软件用流提示符标记其写入命令，SSD根据这些标记分配数据。这要求主机软件仔细区分数据的生命周期并给它们标记，从而真正减少垃圾回收。Stream SSD对OP和DRAM的代价没有什么帮助(因为依旧需要转移数据、且物理块粒度依旧很小)

> ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612192233965-792410754.png)
> ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612192513146-825310759.png)
> OCSSD向主机公开了物理块，消除了设备内垃圾收集开销，并降低了OP和 DRAM 的成本。主机需要决定把数据放到哪个物理块，如何回收块，遇到介质故障时如何解决（而这些问题都和硬件特性有关）。这就相当于没有接口，物理资源直接暴露出来，软件可能需要频繁更新(不再模块化，硬件改变软件就要改变)

#### ZNS

一些ZNS相关知识：

ZNS使用状态机来确定可写zone，使用一个写指针来确保zone内顺序写
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612193220417-333190586.png)

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612194247591-2103696286.png)
ZNS SSD的区域的可写容量可以小于区的实际容量(它规定一些LBA是不可写的)，来适应SMR HDD的行业标准。

ZNS可以按需要限制active zone的数量
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612195204581-294234931.png)
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612195322840-1023506186.png)
最大活动区域的限制是基于SSD的介质特性(program disturb)

### 方法设计

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614235935801-658670902.png)

ZNS接口的做法：

新的责任划分、新的粒度

* 暴露出闪存擦除块边界和写入顺序规则(flash erase block boundaries and write-ordering rules),要求主机软件以擦除块为粒度解决数据管理的问题(不支持随机写，同时主机软件来负责显式擦除)从而不再要求SSD提供垃圾回收机制和预留空间
* 同时降低DRAM需求(传统的FTL闪存转换层需要1GB的DRAM才能映射1TB的NAND闪存，这是因为粒度为4Kb，ZNS每个区域数百M,当然相应的映射所需的数据量也会多一些，ZNS不是逻辑块的一维数组，而是将多个逻辑块分为一个区域(zone)，zone和物理块是对齐的，区域中的逻辑块可以随机读但必须顺序写，且只允许擦除写)，但SSD仍负责管理SSD内的介质可靠性( media reliability)

将 FTL 职责转移给主机的效率不如与存储软件的数据映射和放置逻辑集成（集成到f2fs或者ZenFS）
这一工作基于引入 Linux 内核、fio 基准测试工具、f2fs 文件系统和 RocksDB 键值存储对的 ZNS SSD 的支持 （这些工作已经开源）

#### Hardware

ZNS 的FTL层的一些改变

1. 区域大小调整。区域的写入容量与 SSD 实现的擦除块的大小之间存在直接关联。SSD以擦除块或条带(stripe)为单位提供奇偶校验来防止故障。通常我们主张以提供校验的最小单位来设置区域大小，这样既能在设备级提供数据保护，也能提高主机放置数据的自由度
2. 新的映射表。由于 ZNS SSD 区域写入要求是顺序的,我们完全可以以zone为粒度建立映射表(原本占据SSD上DRAM的大部分)
3. 规划资源。部分写入的擦除块(也就是活动区)会占用资源，ZNS SSD一般设置8~32个活动区

> 擦除块的粒度大小关系到读写性能(写放大)，I/O 队列长度
> I/O queue depths是指在计算机系统中，用于管理输入/输出(I/O)请求的队列中，可以同时排队等待处理的I/O请求的数量。通常，I/O请求会被放置在队列中，等待系统资源(如磁盘、网络等)处理。如果队列中的I/O请求数量过多，可能会导致系统资源繁忙或者饱和，从而影响系统的性能和响应时间。
>在操作系统和存储系统中，通常会设置I/O queue depths的最大值，以控制队列中I/O请求的数量。这个最大值通常是根据系统的硬件配置、工作负载和性能需求等因素进行调整的。如果I/O queue depths过高，可能会导致系统资源的过度使用，从而降低系统的性能和响应时间；如果I/O queue depths过低，可能会导致系统资源的浪费和I/O请求的延迟。
>因此，对于不同的系统和应用场景，需要根据实际情况进行I/O queue depths的设置和调整，以保证系统的性能和响应时间。

zone设置过大可能会导致擦除时出现写放大，如果设置得过小会导致I/O队列过深 (最小不能小于设备提供可靠性支持的最小单元)

#### Software

有三种方式可以使主机软件适配ZNS 接口

1. 主机引入Host-side FTL(HFTL)， HFTL 层类似于 SSD FTL 的职责，**向应用提供了随机写和覆写**，但 HFTL 层只需要管理转换映射和关联的垃圾回收。
2. 文件系统在存储堆栈的更高一级设置zone，来确保其写操作主要是顺序的。
3. 实现端到端的数据放置(应用程序直接控制数据写到ZNS SSD)，可以消除来自文件系统和转换层的间接开销，但也要求应用程序主要顺序写，集成zone支持且为用户提供执行检查、错误检查和备份/还原操作的工具。

> 虽然并非所有的写入都是顺序的（例如超级块和一些元数据写入），但像 f2fs [36], btrfs [53], 和 zfs这样的文件系统可以通过严格的日志结构写入 [43]、浮动超级块 [4] 和类似的功能来弥补差距。
> 主要呈现顺序写入的应用程序有：基于 LSM 树的存储（如 RocksDB）、基于缓存的存储（如 CacheLib [7]）和对象存储（如 Ceph SeaStore [55]。
> ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613223313909-366056414.png)

增加了对四个主要软件项目的支持，以评估 ZNS 的优势。

1. 对Linux内核进行了修改以支持ZNS SSD。
2. 修改了 f2fs 文件系统，以评估在更高级别的存储堆栈层进行区域集成的好处。
3. 修改了 fio [6] 基准测试以支持新添加的 ZNS 特定属性。
4. 开发了ZenFS [25]，这是RocksDB的新型存储后端，允许通过区域控制数据放置，以评估分区存储端到端集成的好处。

其中前三个项目已经支持ZAC/ZBC定义的区域模型，只需要相对较少的更改就可以支持ZNS
ZenFS的架构则是全新的

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613003640076-477714461.png)

##### linux

为Linux 内核设计了Zoned Block Device子系统，来为各种ZNS类设备提供新的接口

ZBD提供了内核内 API 和基于 ioctl 的用户空间 API，来支持设备枚举、区域报告和区域管理（例如区域重置），应用程序通过这些接口发出 I/O 请求

>![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613222447548-564103900.png)

在内核内维护一组有关各个zone的信息，他们由主机维护，当出错时则从特定的磁盘中刷新。

##### fio 和 f2fs

fio和f2fs需要加以更改来识别zone描述表

更改包括

1. 区域容量的相关修改
   * fio需要避免发出超出区域容量的写入 I/O
   * f2fs 按段的粒度管理容量，通常为 2MiB 块，我们要将多个段合并为一个zone进行管理。f2fs的段的状态有:free, open和full，为了支持ZNS，我们又加了两种:unusable(对应zone中unwritable的部分),partial(对应跨越了zone中unwritable和writable部分的段)，这样就解决了zone的容量和段不能对齐的情况。
2. 活动区域限制的相关修改
    * fio不做修改，需要用户去遵守活动区域限制，否则就会出现 I/O 错误
    * f2fs限制可以同时打开的段数为6，但可以减少它来与活动区域限制保持一致。这需要修改 f2fs-tools 来检查区域活动限制。f2fs的元数据是要保存在块设备上的，作者没有直接解决这一问题，而是暴露一部分ZNS SSD的容量作为块设备

##### RocksDB 与 ZenFS

调整常用的键值数据库 RocksDB，以使用 ZenFS 存储后端在分区存储设备上实现端到端数据放置。

ZenFS利用RocksDB的日志结构合并（LSM）树[45]数据结构，用于存储和维护其数据，以及相关的不可变顺序压缩过程。

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614122955433-272117941.png)

LSM树的工作过程：

* $L_0$层在内存中，它保存新的或者更新了的键值对，并定期或者当满的时候将数据传递给下层。刷新之间更新通过预写日志来保存。
* $L_1,L_2, ... ,L_n$层在硬盘中，传递过来的数据将按键值排序，然后以 Sorted String Table (SST) 文件的形式写入到磁盘中。这样可以实现冷热数据分离和**顺序写**。

具体工作原理可以参考[深入浅出分析LSM](https://zhuanlan.zhihu.com/p/415799237)，总的来说就是插入删除和原地更新都直接在$L_0$进行，但查询需要一层一层的往下找。第$L_0$层用树来存，提高插入删除和更新效率，其余层用有序对来存，方便进行归并排序。其数据特征时越上层的数据为越新，当满了之后将其与下层数据归并之后更新下层同时清空本层。

设计原则：

* 先内存再磁盘
* 内存原地更新
* 磁盘追加更新
* 归并保留新值

RockDB需要通过文件系统的API来访问磁盘数据，这个API使用标识符来区分数据单元，基于标识符来实现一系列操作(例如，添加、删除、当前大小、利用率)。

RocksDB 避免了管理文件范围、缓冲和可用空间管理，但也失去了将数据直接放入区域的能力，这阻止了端到端数据放置，从而降低了整体性能。

ZenFS实现了一个最轻量的磁盘文件系统，并与RocksDB的文件系统API集成。

结构如图所示
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614150755671-1444946172.png)

ZenFS 定义了两种类型的区域：日志区域和数据区域。

* 日志区域用于恢复文件系统的状态，并维护超级块数据结构，以及 预写日志 和数据文件到zone的映射。日志存储在专门的日志区域中，一般是设备的前两个非offline的区域。在任何时间点，都会选择其中一个区域作为活动日志区域，保存日志状态更新。日志区域的开头保存一个header，它记录存储序列号（每次初始化新日志区域时递增）、超级块数据结构和当前日志状态的快照。区域的剩余可写容量用于记录日志的更新。

    恢复日志状态有三步

  1. 读取两个日志区域中每个区域的第一个 LBA 以确定每个日志区域的序列号，较大的那个是活动日志区域
  2. 读取活动日志区域的完整header，初始化superblock和日志状态
  3. 对header的日志快照按照记录进行更新

    当ZenFS初始化是就按照上面的步骤进行恢复，之后就可以接受来自RocksDB的数据了。

* 数据区域存储文件内容。数据区域包含多个Extent，它是大小可变、块对齐、顺序写入的一块连续空间，包含与特定标识符关联的数据。Extent的分配和释放会被写入日志，同时extent到zone的映射用内存中的一个数据结构记录下来，当zone中的所有extent被删除，该zone可以被reset。

* 超级块数据结构是从磁盘初始化和恢复 ZenFS 状态时的初始入口点。超级块包含当前实例的唯一标识符、魔术值和用户选项。超级块中的唯一标识符 （UUID） 允许用户识别文件系统，即使系统上块设备枚举的顺序发生变化。(不是很懂)

* 文件大小是数据区域容量的倍数时，能够充分利用所有容量。但我们实现不知道文件大小，这和压缩和压缩过程的结果有关。

    ZenFS 通过允许用户配置数据区域容量限制来解决这一问题。当然如果文件大小不符合设置好的限制时，ZenFS会使用其区域分配算法来利用所有容量。经过检验文件大小增加带来的性能下降并不显著。

* 数据区域选择。ZenFS采用best effort的算法来选择存储RocksDB数据文件的最佳区域。RocksDB 通过在写入文件之前为文件设置写入生命周期标识来分离 WAL 和 SST 文件。ZenFS 首先尝试根据文件的生存期和区域中存储的数据的最大生存期来查找区域。仅当文件的生存期小于区域中存储的最旧数据时，匹配才有效，以避免延长区域中数据的生存期。

* 活动区域限制。ZenFS 必须遵守分区块设备指定的活动区域限制。要运行 ZenFS，至少需要三个活动区域，这些区域分别分配给日志、WAL 和压缩过程。为了提高性能，用户可以提高活动区域的数目。

* 直接 I/O 和缓冲写入。ZenFS 利用对 SST 文件的写入是顺序且不可变的，对 SST 文件绕过内核页面缓存，执行直接 I/O 写入。对于其他文件（如 WAL），ZenFS 把写入暂存到内存，在刷新内存时才填充到磁盘中。

### 效果

实验效果建立在顺序写的限制上

写入吞吐量，提高了1.7-2.7倍，减少了7 − 28%OP

随机读延迟更优(write 依旧是顺序的)
![img](https://img2023.cnblogs.com/blog/3067108/202403/3067108-20240304223859447-1911331186.png)

overwrite测试ZenFS的优化效果明显(183%)，因为overwrite需要较多的垃圾回收

没有写入速度限制的readwhilewriting，ZenFS的写速度优化效果明显

可以发现，XFS和F2FS的读延迟在P99和P99.99差别较大。而F2FS(ZNS)和ZenFS只有randomread测试差别较大，且三组测试延迟均更优，同时F2FS(ZNS)的延迟比ZenFS低

相交7%OP的stream SSD，ZenFS的性能依旧更优

### 可能的改进

## ZNS+: Advanced Zoned Namespace Interface for Supporting In-Storage Zone Compaction

### 背景

尽管LFS具有顺序追加的特性，但因为日志文件系统需要不断地将日志文件的段合并，这意味着大量的拷贝操作，这是相较传统文件系统要多承担的开销。
而ZNS必须和LFS搭配使用(基于顺序写限制)，所以主机需要承担LFS的段压缩开销（本应由SSD的垃圾回收机制代为承担）。
需要研究一种面向ZNS的LFS系统。

> log-on-log problem
> Our work investigates the impacts to performance and endurance in flash when multiple layers of log-structured applications and file systems are layered on top of a log-structured flash device. We show that multiple log layers affects sequentiality and increases write pressure to flash devices through randomization of workloads, unaligned segment sizes, and uncoordinated multi-log garbage collection.

在传统的SSD的应用LFS文件系统就是一个log-on-log问题。因为SSD也按照先写再垃圾回收的方式运行，所以也是一个log系统。上层LFS文件系统的段压缩（垃圾回收）对于SSD来说只是正常的写操作，进一步引发下层的垃圾回收，总的写放大倍率将会是上层的写放大倍率乘上下层的写放大倍率。
而ZNS SSD则没有垃圾回收机制，避免了写放大。

要想利用SSD的闪存芯片并行性需要增大zone的大小（因为每个zone是隔离的，如果zone只包含一个channels，那么对zone的写入就没有并行性了），而zone越大段越大，压缩代价就越大

SSD中：channels(并行闪存控制器) > flash chips > erase block > page > typical logical block size(4KB)

映射到page上的一段连续的logical block称为chunk
zone中的并行闪存芯片的数目称为$D_{zone}$
分别位于不同闪存芯片上的逻辑连续的若干chunk称为条带$stripe$

![img](https://img2023.cnblogs.com/blog/3067108/202403/3067108-20240311165400258-709078669.png)
一个zone包含一个或多个FBG(flash block group),一个FBG要实现并行需要包含多个来自不同并行闪存芯片的擦除块（通常FBG包含不同闪存芯片上块偏移相同的块），而一个条带上包含多个chunk(通常条带包含来自不同擦除块的page偏移相同的chunk)

chunk是copyback操作的基本单位，而同一flash chip内的chunk的copyback操作开销会得到优化

F2FS有6种段(hot、warm、cold 的 node 和 data)每种段同时至多打开一个。
node块包含一个data块的索引，data块则包含目录或用户文件数据。
hot或warm段中的cold块在段压缩过程中会被转移到cold段。
F2FS同时支持append logging和thread logging

⚠️thread logging比直接的段压缩性能更好：虽然要拷贝的块数是一样的(internal plugging)，但internal plugging可以在闪存芯片闲置时在后台进行，且要修改的元数据更少。
但同时thread logging也有一些缺点：thread logging写只能在与要写的数据具有相同类型的脏段上写(否则就会出现不同类型的数据混合在某个段的情况)，但段压缩则可以从所有脏段中找到有效块最少的段。其次，thread logging没办法把cold data移动到cold segement中，这样段中cold data每次参与到internal plugging中。
其次thread logging的回收不方便，当文件系统回收某一个chunk时，可能并不会写入checkpoint，这样在thread logging眼中该chunk仍是有效的，依旧会参与到internal plugging中。

### 方法设计

ZNS+ 接口支持 zone内部压缩(IZC) 和 稀疏顺序覆写

* IZC：
  * 压缩操作可以细分为四个subtask: 👉 待压缩段选择，分配块保存结果、数据拷贝、元数据更新。其中数据拷贝的工作可以交给SSD而不是主机
  ![img](https://img2023.cnblogs.com/blog/3067108/202403/3067108-20240311190218301-501952618.png)
  * 在发送读磁盘请求之前必须分配内存页，这可能会导致page frame回收。同时因为LBA在磁盘上大概率是不连续的，需要依次发送多个读请求（尽管它们可能在同一个闪存芯片上），没法综合所有的读请求来利用闪存芯片的并行性。（例如在图a中每次请求可以并行地读两个闪存芯片，但闪存芯片0在第一次读请求到第四次读请求之间依旧有idle）
  * 而写阶段需要所有读请求都完成才会开始，这样当一个读请求完成后，在内存中的数据不会立刻写入到磁盘中，这段时间SSD也是idle的
  * 而IZC不需要将数据从磁盘拷贝到内存中，同时会更有效率地安排读写操作(利用并行性和copyback操作)，zone_compaction请求是异步的且会重排读写请求
* 稀疏顺序覆写：
  * 有一种名为 threaded logging的回收机制，该机制通过随机产生覆写操作，向脏段的无效空间中写新数据，来代替传统的段回收。但ZNS要求顺序写，所以不能直接通过该机制减少压缩开销。ZNS+选择松弛顺序写的限制，引入稀疏顺序覆写，threaded logging的稀疏顺序覆写在ZNS+中是被允许的，我们可以在覆写请求之间插入有效区域的顺序写，将稀疏顺序写转化真正的顺序写。
  * thread logging的覆写可以很容易与正常的写指令区分开来，因为覆写的LBA将会位于WA之前。收到覆写指令后需要对要跳过的有效块进行判断，为了减少这部分延迟，引入tl_open指令来提前创建闪存芯片中的有效块的位图。
  * F2FS的threaded logging并不直接写入原来的FBG，而是分配一个新的LogFBG，在LogFBG上进行threaded logging写，而且有效块可以用copyback操作来拷贝。
    ![img](https://img2023.cnblogs.com/blog/3067108/202403/3067108-20240311221120609-918088273.png)
    完成一次threaded logging写之后，如果后面还有有效块，可以顺次将其也拷贝到Log FBG
    中，这样可以提前将WP移动到合适的位置
    如果一个并行闪存芯片上的低地址有效块都被拷贝了，而高地址块有效的话，可以顺次将其拷贝到Log FBG中，因为这满足每个闪存芯片上的顺序写要求，同样可以为WP的移动做准备（例如当WP还在chunk1时，就可以将chunk3的内容拷贝到Log FBG中了）

文件系统引入两项技术来与ZNS配合

* 回写感知的块分配：
    回拷操作用于在同一闪存plane内的闪存页面之间复制数据，而无需进行片外数据传输，已经应用到了标准NAND接口中。新的文件系统在块分配时采用放置策略来利用回拷操作(将copy的源逻辑块和目的逻辑块映射到同一个闪存芯片上)
    ![img](https://img2023.cnblogs.com/blog/3067108/202403/3067108-20240311223728440-75688065.png)
    核心就是在目标段所在的FBG中分配恰好能填满的空间（一般情况下），之后先尽可能地对chunk进行copyback（chip ID可以在map机制中可以很容易地得到），之后用read&write指令将剩下的块填到分配的空间中去
* 混合段压缩方式：使文件系统能够分析开销判断该采用IZC还是稀疏顺序覆写来回收段
    为了减少pre-valid 块，引入周期性设置checkpoint的机制，当pre-valid块的数目大于与之 $θ_{PI}$ 时对元数据进行更新
    引入回收代价计算模型，对开销进行分析

### 效果


### 可能的改进

thread logging 具有一些缺点，例如cold data移动的问题和pre-valid block的问题，或许可以针对这些缺点进行改进

## eZNS: An Elastic Zoned Namespace for Commodity ZNS SSDs

### 背景



### 方法设计

zone管理器：管理zone分配和活动资源

分层I/O调度器：读拥塞控制、写权限控制

### 效果

### 可能的改进
