# 阅读记录

## 背景

传统接口的一些特性：

1. 传统的SSD和主机软件的功能划分:![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611130607424-554407542.png)其中SSD的FTL层(Flash Translation Layer，闪存翻译层)需要有映射和垃圾回收机制且SSD要有一定的预留空间
2. 块接口和当前存储设备特性(闪存颗粒)之间存在明显的不匹配，这种不匹配主要体现在我们可以将块写入闪存(接口决定的)但在擦除时必须以更大的粒度擦除闪存(闪存颗粒的性质决定的)。传统SSD使用块接口，导致其要想减少垃圾回收相应的主机软件会较为复杂，用于页面映射的DRAM较大，预留空间也较大(为了减少垃圾回收)，且仍然可能导致吞吐量限制 [17]、写入放大 [3]、性能不可预测性 [33， 64] 和高尾延迟 [16]

* 逻辑块的大小一般是一页，一个擦除块可能包含多个页![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611134729590-1304353693.png)![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611142220405-1672501370.png)
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614175145312-805185323.png)

### SSD

[SSD特性的来源](https://www.tonguebusy.com/a/peixun/xinxi/03-we-q-w-06.html)

为了给基于闪存的设备一个块接口，我们需要FTL层来让设备支持随机"覆写"，映射，垃圾回收，均匀使用闪存

一些缺点：
> 闪存颗粒不支持"覆写"，它只能将数据擦除后重新写，所以逻辑块的写入实际上是写入到一个没有数据的地方，且需要旧的不会用到的数据所在的擦除块被擦除来腾出空间。当擦除块中有的数据用不到了，有的数据还需要使用，那么我们得将需要使用的数据转移到其他擦除块中，把其他擦除块中要擦除的数据转移到那个将要擦除的块统一擦除，为此我们需要预留一部分空间(7%或28%)来方便转移数据。![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611142321342-1853616515.png)
> 首先谁也不能保证某些数据将来一定用不到了，垃圾回收机制只能按照某种标准来决定回收哪些数据，这会带来操作性能的不确定性
> 其次剩余容量越小，垃圾回收就会越频繁，而垃圾回收需要转移数据和初始化擦除块，这会带来一定的写放大
> 再次SSD的使用寿命等同于Block的擦写次数，为了提升总体寿命，SSD的主控芯片会尽可能的让每个Block的擦写次数均匀。这就需要擦写特定的块，也会导致一定的写放大。
> 再再次由于逻辑块的粒度较小，为了支持映射需要较大的DRAM来保存页表
> 最后是预留空间的代价，它不能用来存数据

现有的两种改进措施

1. Stream SSD
2. Open-Channel SSD

Stream SSD将传入数据分到不同的擦除块，从而提升整体的性能和寿命。做法是主机软件用流提示符标记其写入命令，SSD根据这些标记分配数据。这要求主机软件仔细区分数据的生命周期并给它们标记，从而真正减少垃圾回收。Stream SSD对OP和DRAM的代价没有什么帮助(因为依旧需要转移数据、且物理块粒度依旧很小)

> ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612192233965-792410754.png)
> ![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612192513146-825310759.png)
> OCSSD向主机公开了物理块，消除了设备内垃圾收集开销，并降低了OP和 DRAM 的成本。主机需要决定把数据放到哪个物理块，如何回收块，遇到介质故障时如何解决（而这些问题都和硬件特性有关）。这就相当于没有接口，物理资源直接暴露出来，软件可能需要频繁更新(不再模块化，硬件改变软件就要改变)

### ZNS

一些ZNS相关知识：

ZNS使用状态机来确定可写zone，使用一个写指针来确保zone内顺序写
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612193220417-333190586.png)

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612194247591-2103696286.png)
ZNS SSD的区域的可写容量可以小于区的实际容量(它规定一些LBA是不可写的)，来适应SMR HDD的行业标准。

ZNS可以按需要限制active zone的数量
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612195204581-294234931.png)
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230612195322840-1023506186.png)
最大活动区域的限制是基于SSD的介质特性(program disturb)

## 方法设计

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614235935801-658670902.png)

ZNS 接口同时影响了SSD的硬件、固件和主机软件
如何支持 ZNS 接口
f2fs和RocksDB的修改版本如何利用 ZNS SSD 来实现更高的吞吐量和更低的尾部延迟

ZNS接口的做法：

新的责任划分、新的粒度

* 暴露出闪存擦除块边界和写入顺序规则(flash erase block boundaries and write-ordering rules),要求主机软件以擦除块为粒度解决数据管理的问题(不支持随机写，同时主机软件来负责显式擦除)从而不再要求SSD提供垃圾回收机制和预留空间
* 同时降低DRAM需求(传统的FTL闪存转换层需要1GB的DRAM才能映射1TB的NAND闪存，这是因为粒度为4Kb，ZNS每个区域数百M,当然相应的映射所需的数据量也会多一些，ZNS不是逻辑块的一维数组，而是将多个逻辑块分为一个区域(zone)，zone和物理块是对齐的，区域中的逻辑块可以随机读但必须顺序写，且只允许擦除写)，但SSD仍负责管理SSD内的介质可靠性( media reliability)

将 FTL 职责转移给主机的效率不如与存储软件的数据映射和放置逻辑集成（我们提倡的方法） ？？？
这一工作基于引入 Linux 内核、fio 基准测试工具、f2fs 文件系统和 RocksDB 键值存储对的 ZNS SSD 的支持 （这些工作已经开源）


#### Hardware

ZNS 接口使 SSD 能够将顺序区域写入转换为不同的擦除块，从而消除接口介质不匹配。（？）

ZNS 的FTL层的一些改变

1. 区域大小调整。区域的写入容量与 SSD 实现的擦除块的大小之间存在直接关联。SSD以擦除块或条带(stripe)为单位提供奇偶校验来防止故障。通常我们主张以提供校验的最小单位来设置区域大小，这样既能在设备级提供数据保护，也能提高主机放置数据的自由度
2. 新的映射表。由于 ZNS SSD 区域写入要求是顺序的,我们完全可以以zone为粒度建立映射表(原本占据DRAM的大部分)
3. 规划资源。部分写入的擦除块(也就是活动区)会占用资源，ZNS SSD一般设置8~32个活动区

> I/O queue depths是指在计算机系统中，用于管理输入/输出(I/O)请求的队列中，可以同时排队等待处理的I/O请求的数量。通常，I/O请求会被放置在队列中，等待系统资源(如磁盘、网络等)处理。如果队列中的I/O请求数量过多，可能会导致系统资源繁忙或者饱和，从而影响系统的性能和响应时间。
>在操作系统和存储系统中，通常会设置I/O queue depths的最大值，以控制队列中I/O请求的数量。这个最大值通常是根据系统的硬件配置、工作负载和性能需求等因素进行调整的。如果I/O queue depths过高，可能会导致系统资源的过度使用，从而降低系统的性能和响应时间；如果I/O queue depths过低，可能会导致系统资源的浪费和I/O请求的延迟。
>因此，对于不同的系统和应用场景，需要根据实际情况进行I/O queue depths的设置和调整，以保证系统的性能和响应时间。

#### Software

1. Host-side FTL(HFTL) HFTL 层类似于 SSD FTL 的职责，但 HFTL 层仅管理转换映射和关联的垃圾回收。
2. File Systems 文件系统需要在存储堆栈的更高一级设置zone，来确保其写操作主要是顺序的。
3. End-to-End Data Placement 要实现端到端的数据放置(应用程序直接控制数据写到ZNS SSD)，和文件系统一样，也要求应用程序主要顺序写，集成zone支持且为用户提供执行检查、错误检查和备份/还原操作的工具。

> 虽然并非所有的写入都是顺序的（例如超级块和一些元数据写入），但像 f2fs [36], btrfs [53], 和 zfs这样的文件系统可以通过严格的日志结构写入 [43]、浮动超级块 [4] 和类似的功能来弥补差距。这些文件系统有效地模仿了 HFTL 逻辑（通过元数据在磁盘上管理 LBA 映射表），同时还实现了垃圾回收以对数据进行碎片整理并为新写入释放空间。尽管 f2fs 和 btrfs 仅支持 ZAC/ZBC 中定义的区域模型。作为这项工作的一部分，我们对f2fs进行了必要的更改，以展示支持ZNS区域模型的相对容易性，并评估其性能。
> 主要呈现顺序写入的应用程序有：基于 LSM 树的存储（如 RocksDB）、基于缓存的存储（如 CacheLib [7]）和对象存储（如 Ceph SeaStore [55]。为了展示端到端集成的好处，我们介绍了ZenFS，一个新的RocksDB分区存储后端，并将其与（1）XFS文件系统和（2）f2fs文件系统在支持ZNS和不支持ZNS的情况下分别。进行比较。

### Implementation

我们增加了对四个主要软件项目的支持，以评估 ZNS 的优势。首先，我们对Linux内核进行了修改以支持ZNS SSD。其次，我们修改了 f2fs 文件系统，以评估在更高级别的存储堆栈层进行区域集成的好处。第三，我们修改了 fio [6] 基准测试以支持新添加的 ZNS 特定属性。第四，我们开发了ZenFS [25]，这是RocksDB的新型存储后端，允许通过区域控制数据放置，以评估分区存储端到端集成的好处。我们描述了在前三个项目[5，42，52]（§4.1）的现有ZAC/ZBC支持的基础上支持ZNS所需的相对较少的更改，最后详细介绍了ZenFS的架构（§4.2）。

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613003640076-477714461.png)

#### linux

设计了Zoned Block Device子系统，来为各种ZNS类设备提供新的接口

ZBD主要提供内核内 API 和基于 ioctl 的用户空间 API，来支持设备枚举、区域报告和区域管理（例如区域重置），进而允许应用程序通过这些接口发出和与底层分区块设备的写入特征一致的 I/O 请求

修改了NVMe驱动程序来增加ZNS支持

>![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613222405861-1536344716.png)![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613222422478-1521121559.png)![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613222447548-564103900.png)![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230613223313909-366056414.png)

我们在内核内维护一组有关各个zone的信息，它们从特定的磁盘中刷新。fio和f2fs需要加以更改来利用这些数据

其中包括

1. 区域容量的相关修改
   * fio只需要避免发出超出区域容量的写入 I/O
   * f2fs 按段的粒度管理容量，通常为 2MiB 块，我们要将多个段合并为一个zone进行管理。f2fs的段的状态有:free, open和full，为了支持ZNS，我们又加了两种:unusable(对应zone中unwritable的部分),partial(对应跨越了zone中unwritable和writable部分的段)，这样就解决了zone的容量和段不能对齐的情况。
2. 活动区域限制的相关修改
    * fio不做修改，因为不符合限制会导致I/O错误
    * f2fs限制可以同时打开的段数为6，我们修改它来与活动区域限制保持一致，同时修改 f2fs-tools 以检查区域活动限制。f2fs的元数据是要保存在块设备上的，我们可以提供一个单独的块设备或者让ZNS SSD的一部分容量使用块接口(加上类似btrfs的写入功能或一个小的转换层) (也许我们还可以修改f2fs来去除这一限制)

#### RocksDB

我们还要调整常用的键值数据库 RocksDB，以使用 ZenFS 存储后端在分区存储设备上执行端到端数据放置。

ZenFS利用RocksDB的日志结构合并（LSM）树[45]数据结构，用于存储和维护其数据，以及相关的不可变顺序压缩过程。

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614122955433-272117941.png)

LSM树的工作过程：$L_0$层在内存中，它保存新的或者更新了的键值对，并定期或者当满的时候将数据传递给下层，中间传递的一些信息通过Write-Ahead Log来保存，$L_1,L_2,\dot,L_n$层在硬盘中，传递过来的数据将按键值排序，然后以 Sorted String Table (SST) 文件的形式写入到磁盘中。这样可以实现冷热数据分离和顺序写。

具体工作原理可以参考[深入浅出分析LSM](https://zhuanlan.zhihu.com/p/415799237)，总的来说就是插入删除和原地更新都直接在$L_0$进行，但查询需要一层一层的往下找。其数据特征时越上层的数据为越新，当满了之后将其与下层数据归并之后更新下层同时清空本层。

设计原则：

* 先内存再磁盘
* 内存原地更新
* 磁盘追加更新
* 归并保留新值

结构如图所示
![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230614150755671-1444946172.png)

RockDB用一个file system wrapper API 将文件系统与存储后端分离,通过wrapper API来访问其磁盘数据。

这个API使用标识符来区分数据单元，用标识符来实现一系列操作(例如，添加、删除、当前大小、利用率)。

RocksDB 避免了管理文件范围、缓冲和可用空间管理，但也失去了将数据直接放入区域的能力，这阻止了端到端数据放置，从而降低了整体性能。

#### ZenFS

ZenFS 定义了两种类型的区域：日志区域和数据区域。日志区域用于恢复文件系统的状态，并维护超级块数据结构，以及 WAL 和数据文件到区域的映射，而数据区域存储文件内容。

数据区域包含多个Extent，它是大小可变、块对齐、顺序写入的一块连续空间，包含与特定标识符关联的数据。Extent的分配和释放会被写入日志。

超级块数据结构是从磁盘初始化和恢复 ZenFS 状态时的初始入口点。超级块包含当前实例的唯一标识符、魔术值和用户选项。超级块中的唯一标识符 （UUID） 允许用户识别文件系统，即使系统上块设备枚举的顺序发生变化。

日志存储在专门的日志区域中，一般是设备的前两个非offline的区域。在任何时间点，都会选择其中一个区域作为活动日志区域，保存日志状态更新。日志区域的开头保存一个header，它记录存储序列号（每次初始化新日志区域时递增）、超级块数据结构和当前日志状态的快照。区域的剩余可写容量用于记录日志的更新。

恢复日志状态有三步

1. 读取两个日志区域中每个区域的第一个 LBA 以确定每个日志区域的序列号，较大的那个是活动日志区域
2. 读取活动日志区域的完整header，初始化superblock和日志状态
3. 对header的日志快照按照记录进行更新

要应用的更新量由区域的状态及其写入指针决定。如果区域处于 O PEN（或 C LOSED）状态，则只有当前写入指针值之前的记录将重做，而如果区域处于 F ULL 状态，则重做标头之后存储的所有记录。请注意，如果区域已满，则在恢复后，将选择并初始化新的活动日志区域来记录日志更新。

初始日志状态由外部实用程序创建和保留，它将日志的初始序列号、超级块数据结构和空快照写入第一个日志区域。

文件大小是数据区域容量的倍数时，能够充分利用所有容量。但我们实现不知道文件大小，这和压缩和压缩过程的结果有关。

ZenFS 通过允许用户配置数据区域容量限制来解决这一问题。当然如果文件大小不符合设置好的限制时，ZenFS会使用其区域分配算法来利用所有容量。经过检验文件大小增加带来的性能下降并不显著。

数据区域选择。ZenFS采用尽力而为的算法来选择存储RocksDB数据文件的最佳区域。RocksDB 通过在写入文件之前为文件设置写入生命周期标识来分离 WAL 和 SST 文件。首次写入文件时，将分配一个数据区域进行存储。ZenFS 首先尝试根据文件的生存期和区域中存储的数据的最大生存期来查找区域。仅当文件的生存期小于区域中存储的最旧数据时，匹配才有效，以避免延长区域中数据的生存期。如果找到多个匹配项，则使用最接近的匹配项。如果未找到匹配项，则分配一个空区域。如果文件填满了已分配区域的剩余容量，则使用相同的算法分配另一个区域。注意，写入生命周期标识提供给任何 RocksDB 存储后端，因此也会传递给其他兼容文件系统，并且可以与具有流支持的 SSD 一起使用。通过使用 ZenFS 区域选择算法和用户定义的可写容量限制，未使用的区域空间或空间放大保持在 10% 左右。

活动区域限制。ZenFS 必须遵守分区块设备指定的活动区域限制。要运行 ZenFS，至少需要三个活动区域，这些区域分别分配给日志、WAL 和压缩过程。为了提高性能，用户可以控制并发压缩的数量。我们的实验表明，通过限制并发压缩的数量，RocksDB 可以处理少至 6 个具有受限写入性能的活动区域，而超过 12 个活动区域不会增加任何显着的性能优势。

直接 I/O 和缓冲写入。ZenFS 利用对 SST 文件的写入是顺序且不可变的，对 SST 文件绕过内核页面缓存，执行直接 I/O 写入。对于其他文件（如 WAL），ZenFS 缓冲写入内存，并在缓冲区已满、文件关闭或 RocksDB 请求刷新时刷新缓冲区。如果请求刷新，缓冲区将填充到下一个块边界，并将记录保存了的字节数的extent存储在日志中。填充过程会导致少量的写入放大，但填充过程是 ZenFS和传统文件系统都采用的方式。

### 结论

ZNS 支持基于闪存的更高性能和更低的每字节成本 SSD。通过将管理擦除块内数据组织的责任从 FTL 转移到主机软件，ZNS 消除了设备内 LBA 到页面映射、垃圾收集和过度配置。我们对ZNSspecialized f2fs和RocksDB实现的实验表明，与在相同SSD硬件上运行的传统FTL相比，写入吞吐量，读取尾延迟和写入放大有了实质性的改进

## 效果

![img](https://img2023.cnblogs.com/blog/3067108/202306/3067108-20230611133247738-2060533539.png)
每次2GB（总容量），写4次
后三次写入时，垃圾回收机制会被触发，吞吐量下降，且28%OP的SSD更优，因为垃圾回收开销会更小

优化指标：

* 0 %OP
* 不需要垃圾回收，没有写放大，吞吐量更高

## 可能的改进

ZNS将原本硬件的一些责任抛给了软件，硬件性能更好了，但软件的开销会增加，或许可以对软件进行优化